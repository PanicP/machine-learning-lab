{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b [[0.00227122 0.00227122 0.91627463]\n",
      " [0.01678216 0.01678216 0.04561863]]\n",
      "c [[0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]\n",
      " [0.00022712 0.00022712 0.09162746]\n",
      " [0.00167822 0.00167822 0.00456186]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "class LinClass(object):\n",
    "\n",
    "    # allocate weight and bias arrays and store ref 2 train data/labels\n",
    "    def __init__(self, n_in, n_out, X, T):\n",
    "        self.W = np.ones([n_in, n_out]) * 0.1\n",
    "        self.b = np.ones([1, n_out]) * 0.1\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.N = X.shape[0]\n",
    "\n",
    "    # nomal cross-entropy loss. Fill in your code here!\n",
    "    def loss(self, Y, T):\n",
    "        return 0.0\n",
    "\n",
    "    def dLdb(self, Y, T):\n",
    "        return np.zeros(self.b.shape)\n",
    "\n",
    "    # fill in your code here!\n",
    "    def dLdW(self, X, Y, T):\n",
    "        return np.zeros(self.W.shape)\n",
    "\n",
    "    # softmax: fill in your code here!\n",
    "    def S(self, X):\n",
    "        return (np.exp(X) / np.exp(X).sum())\n",
    "        # return X\n",
    "\n",
    "    # dummy model, fill in your code!\n",
    "    def f(self, X):\n",
    "        # calSoftmax = self.S(X)\n",
    "        # print(calSoftmax)\n",
    "        temp = self.S(X * 10)\n",
    "        # print(temp)\n",
    "        return temp\n",
    "        # return np.zeros([X.shape[0], 10])\n",
    "\n",
    "    # performs a single gradient descent step\n",
    "    # works with any size of X and T\n",
    "    def train_step(self, X, T, eps):\n",
    "        Y = self.f(X)\n",
    "        loss = self.loss(Y, T)\n",
    "        dLdb = self.dLdb(Y, T)\n",
    "        dLdW = self.dLdW(X, Y, T)\n",
    "        self.b -= eps * dLdb  # b(i+1) = b(i) - eps * gradL\n",
    "        self.W -= eps * dLdW  # same\n",
    "        return loss\n",
    "\n",
    "    # perform multiple gradient descent steps and display loss. Does it go down??\n",
    "    def train(self, max_it, eps):\n",
    "        for it in range(0, max_it):\n",
    "            print(\"iut=\", it, \"loss=\", self.train_step(self.X, self.T, eps))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # download MNIST if not present in current dir!\n",
    "    if os.path.exists(\"./mnist.npz\") == False:\n",
    "        print(\"Downloading MNIST...\")\n",
    "        fname = 'mnist.npz'\n",
    "        url = 'http://www.gepperth.net/alexander/downloads/'\n",
    "        r = requests.get(url+fname)\n",
    "        open(fname, 'wb').write(r.content)\n",
    "\n",
    "    # read it into\n",
    "    data = np.load(\"mnist.npz\")\n",
    "    traind = data[\"arr_0\"]\n",
    "    trainl = data[\"arr_2\"]\n",
    "    traind = traind.reshape(60000, 784)\n",
    "\n",
    "    # your code from here!!\n",
    "    # print('traind', traind)\n",
    "\n",
    "    # b\n",
    "    inputX = [[-1, -1, 5], [1, 1, 2]]\n",
    "\n",
    "    # def calSoftmax(x):\n",
    "    #     return (np.exp(x) / np.exp(x).sum())\n",
    "    # print('b', calSoftmax(inputX))\n",
    "    lc = LinClass(784, 10, traind, trainl)\n",
    "    print('b', lc.S(inputX))\n",
    "\n",
    "\n",
    "    #c\n",
    "    print('c', lc.f(inputX))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fca71db0846ed8da79651cc37ae136c353db3e2c5a5b78df660c892d8bb501f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
