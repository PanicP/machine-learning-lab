{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax is :\n",
      " [[0.00246652 0.00246652 0.99506695]\n",
      " [0.21194156 0.21194156 0.57611688]]\n",
      "\n",
      "Checking S...:\n",
      " [1. 1.]\n",
      "\n",
      "f is :\n",
      " [[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
      "\n",
      "Cross-Entropy: 0.6931471805399453\n",
      "\n",
      "dLdB : [ 1.e-11  1.e-11  1.e-11  1.e-11 -5.e-01  1.e-11  3.e-01  1.e-11  2.e-01\n",
      "  1.e-11  1.e-11]\n",
      "\n",
      "dLdW : [-1.00000007e-11 -1.00000007e-11  5.00000034e-11  1.00000007e-11\n",
      "  1.00000007e-11  2.00000014e-11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np ;\n",
    "import requests ;\n",
    "import os ;\n",
    "\n",
    "\n",
    "class LinClass(object):\n",
    "\n",
    "  ## allocate weight and bias arrays and store ref 2 train data/labels\n",
    "  def __init__(self, n_in, n_out, X, T):\n",
    "    self.W = np.ones([n_in,n_out]) * 0.1 ;\n",
    "    self.b = np.ones([1,n_out])* 0.1 ;\n",
    "    self.X = X ;\n",
    "    self.T = T ;\n",
    "    self.N = X.shape[0] ;\n",
    "\n",
    "  # nomal cross-entropy loss. Fill in your code here!\n",
    "  def loss(self, Y, T): \n",
    "    return - (np.log(Y) * T).sum(axis=1).mean() ;\n",
    "\n",
    "  def dLdb(self,Y,T):\n",
    "    return -(np.mean(T-Y, axis=0))\n",
    "\n",
    "  # fill in your code here!\n",
    "  def dLdW(self, X, Y, T):\n",
    "    return -(np.mean(np.outer(T-Y, X), axis=0))\n",
    "    \n",
    "  # softmax: fill in your code here!\n",
    "  def S(self,X):   # X is N,d\n",
    "    e = np.exp(X) ;  # N,d\n",
    "    row_sums = e.sum(axis = 1) ;   # (N, )\n",
    "    row_sums_bc = row_sums.reshape(-1,1) ; # N,1\n",
    "    #row_sums_bc = row_sums.reshape(2,1) ;  # less elegant bec. requires knowledge of rows in X\n",
    "    \n",
    "    return e / row_sums_bc;  # N,d / N,1 = N,d\n",
    "       \n",
    "  # dummy model, fill in your code!\n",
    "  def f(self,X):\n",
    "    return self.S (np.matmul(X, self.W) + self.b) ;\n",
    "\n",
    "  # performs a single gradient descent step\n",
    "  # works with any size of X and T\n",
    "  def train_step(self, X, T, eps):\n",
    "    Y = self.f(X) ;\n",
    "    loss = self.loss(Y,T) ;\n",
    "    dLdb = self.dLdb(Y,T) ;      \n",
    "    dLdW = self.dLdW(X, Y, T) ;  \n",
    "    self.b -= eps * dLdb ;       ## b(i+1) = b(i) - eps * gradL\n",
    "    self.W -= eps * dLdW ;       ## same\n",
    "    return loss ;\n",
    "\n",
    "  # perform multiple gradient descent steps and display loss. Does it go down??\n",
    "  def train(self,max_it,eps):\n",
    "    for it in range(0,max_it):\n",
    "      print (\"iut=\", it, \"loss=\", self.train_step(self.X, self.T, eps)) ;\n",
    "\n",
    "## read it into \n",
    "data = np.load(\"mnist.npz\")\n",
    "traind = data[\"arr_0\"] ;\n",
    "trainl = data[\"arr_2\"] ;\n",
    "traind = traind.reshape(60000,784)\n",
    "\n",
    "# your code from here!!\n",
    "lc = LinClass(784,10,traind,trainl) ;\n",
    "x = np.array([[-1,-1,5],[1,1,2.]]) ;\n",
    "y = lc.S(x) ;\n",
    "print(\"Softmax is :\\n\", lc.S(x)) ;\n",
    "print (\"\\nChecking S...:\\n\", y.sum(axis=1))\n",
    "\n",
    "print (\"\\nf is :\\n\", lc.f(traind[0,:])) ;\n",
    "# small value added because log(0) will give a MathError\n",
    "Y_test = np.array([[0,0,0,0,0.5, 0,0.3, 0, 0.2,0,0]])+0.00000000001 ; \n",
    "T_test = np.array([[0,0,0,0,1. , 0,0  , 0, 0  ,0,0]])\n",
    "print(\"\\nCross-Entropy:\", lc.loss(Y_test, T_test)) ;\n",
    "\n",
    "print(\"\\ndLdB :\", lc.dLdb(Y_test, T_test))\n",
    "\n",
    "print(\"\\ndLdW :\", lc.dLdW(x, Y_test, T_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Nov  7 2022, 14:50:35) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fca71db0846ed8da79651cc37ae136c353db3e2c5a5b78df660c892d8bb501f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
